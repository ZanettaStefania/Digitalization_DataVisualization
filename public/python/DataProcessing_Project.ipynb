{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["XJN2WKEHoL3A","KAltRclgii9s","fjCAbv1UjDqi","7-R8C-0PlH26","XcZlZKD2mEf2","XrFVnUP0mEUU","Umj5vraGnxas"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"XJN2WKEHoL3A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeRe6OtauB-R"},"outputs":[],"source":["import keras\n","import tensorflow as tf\n","import pandas as pd\n","import os\n","import collections\n","\n","import json\n","import io\n","\n","import numpy as np\n","\n","import requests  # Import the requests library"]},{"cell_type":"code","source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OFpLJPkEuEJE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706781170704,"user_tz":-60,"elapsed":20271,"user":{"displayName":"Stefania Zanetta","userId":"07912371028671314432"}},"outputId":"3c16532a-799b-4ade-a8f1-c4ffb4dbf113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# 1. Barchart"],"metadata":{"id":"KAltRclgii9s"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Barchart/barchart.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,ind_type,unit,indic_is,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'ind_type', 'unit', 'indic_is', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,ind_type,unit,indic_is,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Remove letters from the dataset\n","# List of year columns to clean\n","year_columns = [str(year) for year in range(2012, 2024)]\n","\n","# Removing leading/trailing spaces from column names\n","df_split.columns = df_split.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df_split[col] = df_split[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","print(df_split.head())\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Barchart/barchart_processed.csv'\n","df_split.to_csv(new_csv_path, index=False)"],"metadata":{"id":"jmw-mLJIi1-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Linechart"],"metadata":{"id":"fjCAbv1UjDqi"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Linechart/linechart_original.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,indic_is,unit,hhtyp,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'ind_type', 'unit', 'indic_is', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,indic_is,unit,hhtyp,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Linechart/linechart_processed.csv'\n","df_split.to_csv(new_csv_path, index=False)"],"metadata":{"id":"y2U4qTD5jIWl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Treemap"],"metadata":{"id":"-W_u9waljNfB"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Treemap/treemap_no_2009.csv'\n","data = pd.read_csv(file_path)\n","\n","# Replace ':' with NaN\n","data = data.replace(r\"\\s*:\\s*\", np.nan, regex=True)\n","\n","years_columns = [str(year) for year in range(2010, 2020)]\n","\n","# Excluding '2018' from the years as it is not present in the dataset\n","adjusted_years_columns = [year + \" \" for year in years_columns if year != '2018']\n","\n","# Convert year columns to numeric, errors='coerce' will convert non-numeric values to NaN\n","data[adjusted_years_columns] = data[adjusted_years_columns].apply(pd.to_numeric, errors='coerce')\n"],"metadata":{"id":"tfyKlNjhjQvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = data['freq,indic_is,unit,hhtyp,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'indic_is', 'unit', 'hhtyp', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = data.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,indic_is,unit,hhtyp,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()"],"metadata":{"id":"v0dZj_zxjhLA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_split = df_split.drop(['freq'], axis=1)\n","df_split = df_split.drop(['hhtyp'], axis=1)\n","df_split = df_split.drop(['unit'], axis=1)\n","df_split"],"metadata":{"id":"lQHTMp6Ujj24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by 'indic_is' and sum values for each year\n","percentage_summed = df_split.groupby('geo')[adjusted_years_columns].sum().reset_index()\n","percentage_summed"],"metadata":{"id":"PtzuWWHujmg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace NaN values with 0 for the purpose of calculation\n","df_split.fillna(0, inplace=True)\n","\n","# Extract the percentage columns\n","percentage_columns = df_split.columns[:-4]\n","\n","# Convert the percentage columns to numeric values\n","df_split[percentage_columns] = df_split[percentage_columns].apply(pd.to_numeric, errors='coerce')\n","\n","# Normalize the percentages to sum up to 100 for each year and country\n","df_normalized = df_split.copy()"],"metadata":{"id":"XbtAH-fCkDAq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["country_list=df_split['geo'].unique()\n","df_prova=pd.DataFrame()\n","for year in df_split.columns[:-2]:\n","  for count in country_list:\n","    for indic_is in df_split['indic_is'].unique():\n","      value =  percentage_summed.loc[percentage_summed['geo'] == count, year].values[0]\n","      single_per = df_split.loc[(df_split['indic_is'] == indic_is) & (df_split['geo'] == count), year]\n","\n","      if value > 0 :\n","        df_normalized.loc[(df_split['indic_is'] == indic_is) & (df_split['geo'] == count), year] = ((single_per  * 100) / value)"],"metadata":{"id":"YrV_e7dAkH5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Treemap/treemap_normalized.csv'\n","df_normalized.to_csv(new_csv_path, index=False)"],"metadata":{"id":"tMJZsWHdkcW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Treemap/treemap_normalized.csv'\n","data = pd.read_csv(file_path)"],"metadata":{"id":"bEiUeFHMkcW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["euro_treemap = data[data['geo'] == 'EU28']"],"metadata":{"id":"N4bBcxi2kpbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for year in adjusted_years_columns:\n","  df_result = euro_treemap[['indic_is', year]].copy()\n","  # Rename columns to 'name' and 'value'\n","  df_result = df_result.rename(columns={'indic_is': 'name', year: 'value'})\n","  # Add a 'parent' column with a common root, e.g., 'Origin'\n","  df_result['parent'] = 'Origin'\n","  # Create a new row with name = 'Origin' and the other two columns empty\n","  new_row = pd.DataFrame({'name': ['Origin'], 'value': [''], 'parent': ['']})\n","  # Append the new row to df_result\n","  df_result = new_row.append(df_result, ignore_index=True)\n","  # Replace NaN values with 0 in df_result\n","  df_result = df_result.fillna(0)\n","  df_result2=df_result[df_result['value'] != 0]\n","  new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Treemap/treemap_average'+year[0:4]+'.csv'\n","  df_result2.to_csv(new_csv_path, index=False)"],"metadata":{"id":"afePDUr8ky5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Stacked percent barchart"],"metadata":{"id":"7-R8C-0PlH26"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/StackedPercent/stacked_percent.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,indic_is,unit,ind_type,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'indic_is', 'unit', 'ind_type', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,indic_is,unit,ind_type,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Remove letters from the dataset\n","# List of year columns to clean (assuming the dataset includes years 2012 to 2023)\n","year_columns = [str(year) for year in range(2012, 2024)]\n","\n","# Removing leading/trailing spaces from column names (if any)\n","df_split.columns = df_split.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df_split[col] = df_split[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","print(df_split.head())\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/StackedPercent/stacked_percent_processed.csv'\n","df_split.to_csv(new_csv_path, index=False)"],"metadata":{"id":"UaH7edqFlXR2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Heatmap"],"metadata":{"id":"5oP5jHnGlboN"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Heatmap/heatmap_processed_2.csv'\n","df = pd.read_csv(file_path)\n","\n","# Remove letters from the dataset\n","# List of year columns to clean (assuming the dataset includes years 2012 to 2023)\n","year_columns = [str(year) for year in range(2009, 2024)]\n","\n","# Removing leading/trailing spaces from column names (if any)\n","df.columns = df.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df[col] = df[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","df.head()\n","print(df.columns)\n","\n","data_filtered = df.drop('freq', axis=1)\n","data_filtered = data_filtered.drop('unit', axis=1)\n","data_filtered = data_filtered.drop('ind_type', axis=1)\n","\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Heatmap/heatmap_processed_finale.csv'\n","data_filtered.to_csv(new_csv_path, index=False)"],"metadata":{"id":"kWRZ70tAlnLB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Piechart"],"metadata":{"id":"XcZlZKD2mEf2"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Piechart/piechart_start.csv'\n","data = pd.read_csv(file_path)\n","\n","# Convert \":\" to NaN to facilitate checking for missing values\n","data.replace(\":\", float(\"NaN\"), inplace=True)\n","\n","# Ensure the 2013 and 2023 columns are of a suitable type for comparison (e.g., numeric or NaN)\n","data['2013'] = pd.to_numeric(data['2013'], errors='coerce')\n","data['2023'] = pd.to_numeric(data['2023'], errors='coerce')\n","\n","# Group by 'geo' and filter out groups where either 2013 or 2023 columns have NaN values for any 'indic_is'\n","filtered_data = data.groupby('geo').filter(lambda x: x['2013'].notna().all() and x['2023'].notna().all())\n","\n","# Display the first few rows of the filtered dataset to verify the result\n","filtered_data.head()\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Piechart/piechart_processed.csv'\n","filtered_data.to_csv(new_csv_path, index=False)\n","\n","# Count the number of unique 'geo' values in the filtered dataset\n","unique_geo_count = filtered_data['geo'].nunique()\n","unique_geo_count\n"],"metadata":{"id":"wQkUAWPqmL1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7.Bubblemap"],"metadata":{"id":"XrFVnUP0mEUU"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Heatmap/heatmap_processed_finale.csv'\n","data = pd.read_csv(file_path)\n","data.head()\n","\n","# Mapping of two-letter \"geo\" codes to three-letter codes provided by the user\n","code_mapping = {\n","    \"AL\": \"ALB\", \"AT\": \"AUT\", \"BA\": \"BIH\", \"BE\": \"BEL\", \"BG\": \"BGR\", \"CH\": \"CHE\",\n","    \"CY\": \"CYP\", \"CZ\": \"CZE\", \"DE\": \"DEU\", \"DK\": \"DNK\", \"EE\": \"EST\", \"EL\": \"GRC\",\n","    \"ES\": \"ESP\", \"FI\": \"FIN\", \"FR\": \"FRA\", \"HR\": \"HRV\", \"HU\": \"HUN\", \"IE\": \"IRL\",\n","    \"IS\": \"ISL\", \"IT\": \"ITA\", \"LT\": \"LTU\", \"LU\": \"LUX\", \"LV\": \"LVA\", \"MK\": \"MKD\",\n","    \"MT\": \"MLT\", \"NL\": \"NLD\", \"NO\": \"NOR\", \"PL\": \"POL\", \"PT\": \"PRT\", \"RO\": \"ROU\",\n","    \"RS\": \"SRB\", \"SE\": \"SWE\", \"SI\": \"SVN\", \"SK\": \"SVK\", \"TR\": \"TUR\", \"GB\": \"GBR\"\n","}\n","\n","# Map the \"geo\" column to a new \"code\" column using the provided mapping\n","data['code'] = data['geo'].map(code_mapping)\n","\n","# Display the updated DataFrame to verify the new \"code\" column\n","data.head()\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Bubblemap/bubblemap_processed_finale.csv'\n","data.to_csv(new_csv_path, index=False)\n"],"metadata":{"id":"xogk-LodmTdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Small multiple barchart"],"metadata":{"id":"Umj5vraGnxas"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/SmallMultiple/smallmultiple_original.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,ind_type,indic_is,unit,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'ind_type', 'indic_is', 'unit', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,ind_type,indic_is,unit,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/SmallMultiple/smallmultiple_processed.csv'\n","df_split.to_csv(new_csv_path, index=False)\n","\n","\n","file_path = '/content/drive/MyDrive/Colab/DataViz/SmallMultiple/smallmultiple_processed.csv'\n","df = pd.read_csv(file_path)\n","\n","year_columns = [\"2016\", \"2018\", \"2021\", \"2023\"]\n","# Removing leading/trailing spaces from column names (if any)\n","df_split.columns = df_split.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df_split[col] = df_split[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","print(df_split.head())\n","\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/StackedPercent/smallmultiple_noletters.csv'\n","df_split.to_csv(new_csv_path, index=False)"],"metadata":{"id":"RS1Vvpdin55i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Ridgeline"],"metadata":{"id":"AW0Y5vhamYcP"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Ridgeline/ridgeline_original.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,indic_is,unit,ind_type,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'indic_is', 'unit', 'ind_type', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,indic_is,unit,ind_type,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Remove letters from the dataset\n","# List of year columns to clean (assuming the dataset includes years 2012 to 2023)\n","year_columns = [str(year) for year in range(2011, 2020)]\n","\n","# Removing leading/trailing spaces from column names (if any)\n","df_split.columns = df_split.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df_split[col] = df_split[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","print(df_split.head())\n","\n","df_split = df_split[df_split['unit'] == 'PC_IND']\n","\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Ridgeline/ridgeline_processed.csv'\n","df_split.to_csv(new_csv_path, index=False)"],"metadata":{"id":"yYYHzoqamcsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Choropleth Map"],"metadata":{"id":"vKRX954kmfVw"}},{"cell_type":"code","source":["# File path\n","file_path = '/content/drive/MyDrive/Colab/DataViz/Map/map_original.csv'\n","df = pd.read_csv(file_path)\n","\n","# Splitting the 'freq,ind_type,unit,indic_is,geo\\TIME_PERIOD' column into separate columns\n","split_columns = df['freq,indic_is,ind_type,unit,geo\\TIME_PERIOD'].str.split(',', expand=True)\n","\n","# Renaming the columns as per the user's request\n","split_columns.columns = ['freq', 'indic_is', 'ind_type', 'unit', 'geo']\n","\n","# Joining the new columns with the original dataframe\n","df_split = df.join(split_columns)\n","\n","# Dropping the original combined column\n","df_split = df_split.drop('freq,indic_is,ind_type,unit,geo\\TIME_PERIOD', axis=1)\n","\n","# Displaying the modified dataframe\n","df_split.head()\n","\n","# Remove letters from the dataset\n","# List of year columns to clean (assuming the dataset includes years 2012 to 2023)\n","year_columns = [\"2015\", \"2016\", \"2017\", \"2019\"]\n","\n","# Removing leading/trailing spaces from column names (if any)\n","df_split.columns = df_split.columns.str.strip()\n","\n","# Converting year columns to strings and then removing any letters\n","for col in year_columns:\n","    df_split[col] = df_split[col].astype(str).replace(to_replace=r'[a-zA-Z]', value='', regex=True).str.strip()\n","\n","print(df_split.head())\n"],"metadata":{"id":"KJHxO5TRmkiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["countryMapping = {\n","    \"AL\": \"Albania\",\n","    \"AT\": \"Austria\",\n","    \"BA\": \"Bosnia and Herzegovina\",\n","    \"BE\": \"Belgium\",\n","    \"BG\": \"Bulgaria\",\n","    \"CH\": \"Switzerland\",\n","    \"CY\": \"Cyprus\",\n","    \"CZ\": \"Czech Republic\",\n","    \"DE\": \"Germany\",\n","    \"DK\": \"Denmark\",\n","    \"EE\": \"Estonia\",\n","    \"EL\": \"Greece\",\n","    \"ES\": \"Spain\",\n","    \"EU27_2020\": \"European Union\",\n","    \"FI\": \"Finland\",\n","    \"FR\": \"France\",\n","    \"HR\": \"Croatia\",\n","    \"HU\": \"Hungary\",\n","    \"IE\": \"Ireland\",\n","    \"IS\": \"Iceland\",\n","    \"IT\": \"Italy\",\n","    \"LT\": \"Lithuania\",\n","    \"LU\": \"Luxembourg\",\n","    \"LV\": \"Latvia\",\n","    \"MK\": \"North Macedonia\",\n","    \"MT\": \"Malta\",\n","    \"NL\": \"Netherlands\",\n","    \"NO\": \"Norway\",\n","    \"PL\": \"Poland\",\n","    \"PT\": \"Portugal\",\n","    \"RO\": \"Romania\",\n","    \"RS\": \"Serbia\",\n","    \"SE\": \"Sweden\",\n","    \"SI\": \"Slovenia\",\n","    \"SK\": \"Slovakia\",\n","    \"TR\": \"Turkey\",\n","    \"UK\": \"United Kingdom\"\n","}\n","\n","# Create a dataframe from the dictionary\n","df_country_mapping = pd.DataFrame(list(countryMapping.items()), columns=['geo', 'Country'])\n","\n","# Display the dataframe\n","print(df_country_mapping)"],"metadata":{"id":"uwolRqKBmsZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the dataframes using the 'code' and 'geo' columns\n","merged_df = pd.merge(df_country_mapping, df_split, how='inner')"],"metadata":{"id":"EHw4k63Umv1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset2015 = pd.DataFrame()\n","dataset2015['name'] = merged_df2['Country']\n","dataset2015['code'] = merged_df2['code']\n","dataset2015['pop'] = merged_df2['2015']\n","# Replace ':' with NaN\n","dataset2015 = dataset2015.replace(r\"\\s*:\\s*\", np.nan, regex=True)\n","\n","dataset2016 = pd.DataFrame()\n","dataset2016['name'] = merged_df2['Country']\n","dataset2016['code'] = merged_df2['code']\n","dataset2016['pop'] = merged_df2['2016']\n","# Replace ':' with NaN\n","dataset2016 = dataset2016.replace(r\"\\s*:\\s*\", np.nan, regex=True)\n","\n","dataset2017 = pd.DataFrame()\n","dataset2017['name'] = merged_df2['Country']\n","dataset2017['code'] = merged_df2['code']\n","dataset2017['pop'] = merged_df2['2017']\n","# Replace ':' with NaN\n","dataset2017 = dataset2017.replace(r\"\\s*:\\s*\", np.nan, regex=True)\n","\n","dataset2019 = pd.DataFrame()\n","dataset2019['name']= merged_df2['Country']\n","dataset2019['code']= merged_df2['code']\n","dataset2019['pop']= merged_df2['2019']\n","# Replace ':' with NaN\n","dataset2019 = dataset2019.replace(r\"\\s*:\\s*\", np.nan, regex=True)"],"metadata":{"id":"DI6tjnUgnJ8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Map/map_processed_2015.csv'\n","dataset2015.to_csv(new_csv_path, index=False)\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Map/map_processed_2016.csv'\n","dataset2016.to_csv(new_csv_path, index=False)\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Map/map_processed_2017.csv'\n","dataset2017.to_csv(new_csv_path, index=False)\n","# Saving the new CSV file\n","new_csv_path = '/content/drive/MyDrive/Colab/DataViz/Map/map_processed_2019.csv'\n","dataset2019.to_csv(new_csv_path, index=False)"],"metadata":{"id":"FI5TaLaunTCw"},"execution_count":null,"outputs":[]}]}